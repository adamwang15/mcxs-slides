\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{nopageno}
\usepackage[colorlinks=false]{hyperref}
\hypersetup{colorlinks=true}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{color}
\usepackage[top=1cm, bottom=2cm, left=1cm, right=1cm]{geometry}
\usepackage[none]{hyphenat}
\usepackage{changepage}
\usepackage{fancyhdr}
\usepackage{cmbright}


\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
%\newcommand{\helv}{\fontfamily{pag}\fontsize{10}{11}\selectfont}
%\rhead{Share\LaTeX}
%\lhead{Guides and tutorials}
%\lfoot{\rule{5cm}{.1pt}\\  Tomasz Wo\'zniak $\bullet$ e-mail: \href{mailto:tomasz.wozniak@unimelb.edu.au}{tomasz.wozniak@unimelb.edu.au} $\bullet$ website: \href{https://gitlab.com/tomaszwozniak}{gitlab.com/tomaszwozniak}\hspace{4cm} \thepage}



\begin{document}



\bigskip\noindent\textbf{\LARGE Macroeconometrics: Test 1}

\bigskip\noindent Examples of solutions prepared by Tomasz (in this note I explain a bit more than was required in the test)

\smallskip\noindent\rule{5cm}{.1pt}

\normalsize

\begin{adjustwidth}{1cm}{1cm}

\begin{description}
\bigskip\item[Exercise 1. (2.5 points)] Consider the following autoregression for a random variable $y_t$ with the scalar parameters $\alpha$ and $\sigma^2$ and normally distributed error term $u_t$:
\begin{align}
y_t &= \alpha y_{t-2} + u_t\\
u_t | y_{t-2} &\sim iid\mathcal{N}\left(0, \sigma^2\right)
\end{align}
\begin{itemize}
\item Derive autocorrelations at lags 0, 1, 2, 3, and 4 implied by this model. Show your workings. State the assumptions you are applying to get your result.
\item Given the derived autocorrelations, comment in two sentences about what memory patterns this model implies about the data generated by the corresponding data-generating process.
\end{itemize}

\bigskip\item[Proposed solution.]  The process in eq (1) has a zero constant term and, thus, $E[y_t] = 0$. Multiply both sides by $y_{t-s}$ and apply the expectation:
\begin{align}
E[y_ty_{t-s}] &= E[(\alpha y_{t-2} + u_t)y_{t-s}]\\
&= \alpha E[y_{t-2}y_{t-s}] +  E[u_ty_{t-s}]\\
\gamma_s &= \alpha \gamma_{s-2} +  E[u_ty_{t-s}]
\end{align}
Assume stationarity to facilitate $\gamma_{s}=\gamma_{-s}$ (It's OK to bring in an assumption if that leads to a solution). Write out eq (5) for $s = 0,1,2,3,4$:
\begin{align}
s &= 0\\
\gamma_0 &= \alpha \gamma_{-2} +  E[u_ty_{t}]\\
\gamma_0 &= \alpha \gamma_{2} +  \sigma^2 \\[1ex]
s &>1\\
\gamma_s &= \alpha \gamma_{s-2} +  E[u_ty_{t-s}]\\
\gamma_s &= \alpha \gamma_{s-2} \\[1ex]
s &= 1\\
\gamma_1 &= \alpha \gamma_{1} \\
\gamma_1 &= 0\\[1ex]
s &= 2\\
\gamma_2 &= \alpha \gamma_{0} \\
&\Downarrow\\
\gamma_0 &= \frac{\sigma^2}{1-\alpha^2}\\
\gamma_2 &= \alpha\frac{\sigma^2}{1-\alpha^2}
\end{align}

\begin{align}
s &= 3\\
\gamma_3 &= \alpha \gamma_{1} = 0 \\[1ex]
s &= 4\\
\gamma_4 &= \alpha \gamma_{2} = \alpha^2 \frac{\sigma^2}{1-\alpha^2}\\[1ex]
\end{align}
This leads to the autocorrelations:
\begin{align}
\rho_0 &= \frac{\gamma_0}{\gamma_0} = 1\\
\rho_1 &= \frac{\gamma_1}{\gamma_0} = 0\\
\rho_2 &= \frac{\gamma_2}{\gamma_0} = \alpha\\
\rho_3 &= \frac{\gamma_3}{\gamma_0} = 0\\
\rho_4 &= \frac{\gamma_4}{\gamma_0} = \alpha^2
\end{align}
This process has a specific memory pattern where there is no memory at odd lags, that is, zero autocorrelations at odd lags, and memory decaying exponentially at rate $\alpha$ at even lags.
\end{description} 

\newpage
\begin{description}
\item[Exercise 2. (2.5 points)] Consider the autoregression from \textbf{Exercise 1} applied to $T$ observations on variable $y$.
\begin{itemize}
\item Write out the model in a matrix notation.
\item State the distribution of the error term vector explicitly.
\item State the predictive density implied by the model for the dependent variable vector given the explanatory variables.
\item Write out the likelihood function for the model.
\end{itemize}
\item[a pdf of the multivariate normal distribution] for an $N$-random vector $X$ with mean $\mu$ and covariance~$\Sigma$
\begin{align}
X\sim\mathcal{N}_N\left(\mu, \Sigma\right) = (2\pi)^{-\frac{N}{2}}\det(\Sigma)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}\left(X-\mu\right)'\Sigma^{-1}\left(X-\mu\right) \right\}
\end{align} 

\bigskip\item[Proposed solution.] 
Define $T-2\times 1$ column-vectors:
\begin{align}
Y = \begin{bmatrix}y_3 \\ \vdots \\ y_T \end{bmatrix},\qquad X = \begin{bmatrix}y_1 \\ \vdots \\ y_{T-2} \end{bmatrix},\qquad U = \begin{bmatrix}u_3 \\ \vdots \\ u_T \end{bmatrix}
\end{align}
and write out the model as:
\begin{align}
Y &= \alpha X + U \label{eq:armat}
\end{align}
where the error term vector follow a $T-2$-variate normal distribution
\begin{align}
U\mid X &\sim\mathcal{N}_{T-2}\left(\mathbf{0}_{T-2}, \sigma^2 I_{T-2}\right)
\end{align}
Since $Y$ is a linear transformation of a normal vector $U$, according to eq \eqref{eq:armat}, it is also normal:
\begin{align}
Y\mid X &\sim\mathcal{N}_{T-2}\left(\alpha X, \sigma^2 I_{T-2}\right)
\end{align}
The latter equation is the predictive density of $Y$ given $X$, and parameters $\alpha$ and $\sigma^2$ (neglected in the notation above), that determines the form of the likelihood function:
\begin{align}
L\left(Y\mid X, \alpha, \sigma^2\right) = (2\pi)^{-\frac{T-2}{2}}\left(\sigma^2\right)^{-\frac{T-2}{2}}\exp\left\{ -\frac{1}{2\sigma^2}(Y-\alpha X)'(Y-\alpha X)\right\}
\end{align}
\end{description} 


\newpage
\begin{description}
\item[Exercise 3. (2.5 points)] Consider the autoregression from \textbf{Exercise 1} represented in the matrix notation in your answer to \textbf{Exercise 2} Assume the following prior distribution for the parameter~$\alpha$:
\begin{align}
\alpha\mid\underline{\alpha}, \underline{\sigma}^2_\alpha \sim\mathcal{N}\left(\underline{\alpha}, \underline{\sigma}^2_\alpha\right)
\end{align}
where the hyper-parameters $\underline{\alpha}$ and $\underline{\sigma}^2_\alpha$ are assumed to be known.
\begin{itemize}
\item Derive the full-conditional posterior distribution of the parameter $\alpha$ given data, as well as parameter $\sigma^2$ and hyper-parameters $\underline{\alpha}$ and $\underline{\sigma}^2_\alpha$, denoted by $p\left(\alpha\mid data, \sigma^2, \underline{\alpha}, \underline{\sigma}^2_\alpha\right)$. Show your workings.
\end{itemize}

\bigskip\item[Proposed solution.] 
The kernel of the prior distribution is given by:
\begin{align}
\exp\left\{ -\frac{1}{2}(\alpha - \underline{\alpha})' \underline{\sigma}^{-2}_\alpha (\alpha - \underline{\alpha}) \right\}
\end{align}
Bayes rule for the full-conditional posterior is given by:
\begin{align}
p&\left(\alpha\mid Y, X, \sigma^2, \underline{\alpha}, \underline{\sigma}^2_\alpha \right) \propto L\left(Y\mid X, \alpha, \sigma^2\right) p\left(\alpha\mid\underline{\alpha}, \underline{\sigma}^2_\alpha\right)\\
&= \exp\left\{ -\frac{1}{2}(Y-\alpha X)'\sigma^{-2}(Y-\alpha X)\right\}\exp\left\{ -\frac{1}{2}(\alpha - \underline{\alpha})' \underline{\sigma}^{-2}_\alpha (\alpha - \underline{\alpha}) \right\}\\
&= \exp\left\{ -\frac{1}{2}\left[\alpha'(X'\sigma^{-2}X)\alpha - 2\alpha'X'\sigma^{-2}Y + \dots + \alpha'\underline{\sigma}^{-2}_\alpha \alpha - 2\alpha'\underline{\sigma}^{-2}_\alpha \underline{\alpha} + \dots \right]  \right\}\\
&= \exp\left\{ -\frac{1}{2}\left[\alpha'(X'\sigma^{-2}X + \underline{\sigma}^{-2}_\alpha)\alpha - 2\alpha'(X'\sigma^{-2}Y + \underline{\sigma}^{-2}_\alpha \underline{\alpha}) + \dots   \right]  \right\}
\end{align}
Let $\overline{\sigma}^2_\alpha = (X'\sigma^{-2}X + \underline{\sigma}^{-2}_\alpha)^{-1} $. Rewrite the kernel as:
\begin{align}
& \exp\left\{ -\frac{1}{2}\left[\alpha'\overline{\sigma}^{-2}_\alpha\alpha - 2\alpha'\overline{\sigma}^{-2}_\alpha \overline{\sigma}^{2}_\alpha(X'\sigma^{-2}Y + \underline{\sigma}^{-2}_\alpha \underline{\alpha}) + \dots   \right]  \right\}
\end{align}
Let $\overline{\alpha} = \overline{\sigma}^{2}_\alpha(X'\sigma^{-2}Y + \underline{\sigma}^{-2}_\alpha \underline{\alpha})$. Then...
\begin{align}
& \exp\left\{ -\frac{1}{2}\left[\alpha'\overline{\sigma}^{-2}_\alpha\alpha - 2\alpha'\overline{\sigma}^{-2}_\alpha \overline{\alpha} + \dots   \right]  \right\}
\end{align}
in which I recognise the kernel of the following normal distribution:
\begin{align}
\alpha\mid Y, X, \sigma^2, \underline{\alpha}, \underline{\sigma}^2_\alpha &\sim\mathcal{N}\left(\overline{\alpha} ,\overline{\sigma}^{2}_\alpha \right)\\[1ex]
\overline{\sigma}^{2}_\alpha &= (X'\sigma^{-2}X + \underline{\sigma}^{-2}_\alpha)^{-1}\\
\overline{\alpha} &= \overline{\sigma}^{2}_\alpha (X'\sigma^{-2}Y + \underline{\sigma}^{-2}_\alpha \underline{\alpha})
\end{align}
\end{description} 


\newpage
\begin{description}
\item[Exercise 4. (2.5 points)] Consider the prior distribution for the autoregressive parameter assumed in \textbf{Exercise 3} in the context of applying the model to fortnightly (bi-weekly) data on cash rate target (Australian interest rate).
\begin{itemize}
\item Propose the values for the hyper-parameters $\underline{\alpha}$ and $\underline{\sigma}^2_\alpha$ that would reflect a weak presumption that the time series is unit-root non-stationary. 
\item Write two sentences to make a case for the proposed values for each of the hyper-parameters. 
\end{itemize}

\bigskip\item[Proposed solution.] 
Rewrite the model in a lag polynomial form:
\begin{align}
(1 - \alpha L^2)y_t = u_t
\end{align}
Use the lag polynomial to construct a characteristic polynomial in $z$:
\begin{align}
1 - \alpha z^2 = 0
\end{align}
Consider a unit root $z=1$. The value of $\alpha$ implied by the unit root is:
\begin{align}
1 - \alpha 1^2 &= 0\\
\alpha &= 1
\end{align}
Note: all of the above is not required as long as you state that:

\smallskip\noindent The value of the parameter $\alpha$ that implies the unit root in $y_t$ is 1. Therefore, to reflect the presumption that the time series is unit-root non-stationary in the prior specification we set $\underline{\alpha} = 1$.

\smallskip\noindent To incorporate a weak conviction that the series is unit-root non-stationary in the prior distribution, I set a fairly large value of the prior shrinkage hyper-parameter $\underline{\sigma}^2_\alpha = 4$. As long as the prior distribution is centred at the unit-root non-stationary process, this value of the shrinkage implies e.g. that the stationarity region $\alpha\in(-1, 1)$ is covered by the 68\% confidence region of the prior distribution (the mean $\pm$ one standard deviation). 

%\begin{align}
%
%\end{align}
\end{description} 




\end{adjustwidth}

\end{document}


