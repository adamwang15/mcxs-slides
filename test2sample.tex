\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{nopageno}
\usepackage[colorlinks=false]{hyperref}
\hypersetup{colorlinks=true}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{color}
\usepackage[top=1cm, bottom=2cm, left=1cm, right=1cm]{geometry}
\usepackage[none]{hyphenat}
\usepackage{changepage}
\usepackage{fancyhdr}
\usepackage{cmbright}


\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
%\newcommand{\helv}{\fontfamily{pag}\fontsize{10}{11}\selectfont}
%\rhead{Share\LaTeX}
%\lhead{Guides and tutorials}
%\lfoot{\rule{5cm}{.1pt}\\  Tomasz Wo\'zniak $\bullet$ e-mail: \href{mailto:tomasz.wozniak@unimelb.edu.au}{tomasz.wozniak@unimelb.edu.au} $\bullet$ website: \href{https://gitlab.com/tomaszwozniak}{gitlab.com/tomaszwozniak}\hspace{4cm} \thepage}



\begin{document}



\bigskip\noindent\textbf{\LARGE Macroeconometrics: Test 2 sample questions}

%\bigskip\noindent Your student ID: \dots

\smallskip\noindent\rule{5cm}{.1pt}

\normalsize

\begin{adjustwidth}{1cm}{1cm}

\begin{description}
\bigskip\item[Exercise 1.] Consider the following autoregression for a detrended random variable $y_t$ with the scalar parameters $\alpha\in(-1,1)$ and $\sigma^2$, autocorrelated error term $\epsilon_t$, and normally distributed error term $u_t$:
\begin{align}
y_t &= \beta_0 + \beta_1 t + \epsilon_t\\
\epsilon_t &= \alpha \epsilon_{t-2} + u_t\\
u_t | Y_{t-1} &\sim iid\mathcal{N}\left(0, \sigma^2\right)
\end{align}
where $Y_{t-1}$ collects all the observations on variable $y$ up to time $t-1$.

\begin{itemize}
\item Compute the unconditional expected value of $y_t$, denoted by $E[y_t]$.
\item Derive autocorrelations at lags 0, 1, and 2 implied by this model. Show your workings. State the assumptions you are applying to get your result.
\item Comment in a sentence about the memory patterns this model implies about the data.
\end{itemize}


\bigskip\item[Exercise 2.] Consider the following stationary VAR(1) model for an $N$-vector $\mathbf{y}_t$ with $N\times N$ parameter matrices $\mathbf{A}$ and $\mathbf{\Sigma}$ denoting the autoregressive and covariance matrices respectively and normally distributed error term $\mathbf{u}_t$:
\begin{align}
\mathbf{y}_t &= \mathbf{A} \mathbf{y}_{t-1} + \mathbf{u}_t\\
\mathbf{u}_t &\sim\mathcal{N}_N(\mathbf{0}_N, \mathbf{\Sigma})
\end{align}
\begin{itemize}
\item Derive autocovariance matrices at lags 0, 1, and 2 implied by this model. Show your workings. State the assumptions you are applying to get your result.
\item Comment in a sentence about the memory patterns this model implies about the data.
\end{itemize}


\bigskip\item[Exercise 3.] Consider the following autoregression with $p$ lags for a scalar random variable $y_t$ with the constant term $\mu_0$, autoregressive parameters $\alpha_i$ for $i=1,\dots, p$, and variance $\sigma^2$:
\begin{align}
y_{t} &= \mu_0 + \alpha_1 y_{t-1} + \dots + \alpha_p y_{t-p} + u_t\\
u_t \mid Y_{t-1} &\sim\mathcal{N}\left(0,\sigma^2\right)
\end{align}
\begin{itemize}
\item Write out the model in a matrix notation.
\item State the distribution of the error term vector explicitly.
\item State the predictive density implied by the model for the dependent variable vector given the explanatory variables.
\item Write out the likelihood function for the model.
\end{itemize}
\item[a pdf of the multivariate normal distribution] for an $N$-random vector $\mathbf{X}$ with mean $\boldsymbol\mu$ and covariance~$\mathbf\Sigma$
\begin{align}
\mathbf{X}\sim\mathcal{N}_N\left(\boldsymbol\mu, \mathbf\Sigma\right) = (2\pi)^{-\frac{N}{2}}\det(\mathbf\Sigma)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}\left(\mathbf{X}-\boldsymbol\mu\right)'\mathbf\Sigma^{-1}\left(\mathbf{X}-\boldsymbol\mu\right) \right\}
\end{align} 


\newpage
\bigskip\item[Exercise 4.] Consider the autoregression from \textbf{Exercise 3} represented in the matrix notation. Assume the following prior distribution for the $p+1 \times 1$ vector parameter $\boldsymbol\alpha = (\mu_0, \alpha_1, \dots, \alpha_p)'$:
\begin{align}
\boldsymbol\alpha\mid\underline{\boldsymbol\alpha}, \underline{\sigma}^2_\alpha \sim\mathcal{N}_{p+1}\left(\underline{\boldsymbol\alpha}, \underline{\sigma}^2_\alpha \mathbf{I}_{p+1}\right)
\end{align}
where $\mathbf{I}_K$ is the identity matrix of order $K$, and $\underline{\boldsymbol\alpha}$ is the $(p+1) \times 1$ vector of the prior mean and the scalar hyper-parameter $\underline{\sigma}^2_\alpha$ is the prior variance.
\begin{itemize}
\item Derive the full-conditional posterior distribution of the parameter vector $\boldsymbol\alpha$ given data, as well as parameter $\sigma^2$ and hyper-parameters $\underline{\boldsymbol\alpha}$ and $\underline{\sigma}^2_\alpha$, denoted by $p\left(\boldsymbol\alpha\mid data, \sigma^2, \underline{\boldsymbol\alpha}, \underline{\sigma}^2_\alpha\right)$. Show your workings.
\end{itemize}


\bigskip\item[Exercise 5.] Consider the autoregression from \textbf{Exercise 3} represented in the matrix notation. Assume the following prior distribution for the scalar parameter $\sigma^2$:
\begin{align}
\sigma^2\mid\underline{s},\underline{\nu} &\sim\mathcal{IG}2(\underline{s},\underline{\nu})
\end{align}
where $\underline{s}$ and $\underline{\nu}$ are positive scalar hyper-parameters of the scale and shape respectively.
\begin{itemize}
\item Derive the full-conditional posterior distribution of the parameter $\sigma^2$ given data, as well as parameter $\boldsymbol\alpha$ and hyper-parameters $\underline{s}$ and $\underline{\nu}$, denoted by $p\left(\sigma^2\mid data, \boldsymbol\alpha, \underline{s}, \underline{\nu}\right)$. Show your workings.
\end{itemize}



\item[a pdf of the inverted gamma 2 distribution] for a positive real-values scalar parameter $\sigma^2$ with scale $s$ and shape $\nu$
\begin{align}
\sigma^2\sim\mathcal{IG}2\left(s, \nu\right) = \Gamma\left(\frac{\nu}{2}\right)^{-1}\left(\frac{s}{2}\right)^{\frac{\nu}{2}} \left(\sigma^2\right)^{-\frac{\nu+2}{2}}\exp\left\{ -\frac{1}{2}\frac{s}{\sigma^2} \right\}
\end{align}


\bigskip\item[Exercise 6.] Consider the autoregression from \textbf{Exercise 3} represented in the matrix notation and the prior for the parameter vector $\boldsymbol\alpha$ from \textbf{Exercise 4}. Suppose that you want to estimate the shrinkage hyper-parameter $\underline{\sigma}^2_\alpha$ of the prior distribution for parameter $\boldsymbol\alpha$. For that purpose, assume the following inverted gamma 2 prior distribution for this hyper-parameter:
\begin{align}
\underline{\sigma}^2_\alpha \mid \underline{s}_\sigma,\underline{\nu}_\sigma &\sim\mathcal{IG}2(\underline{s}_\sigma,\underline{\nu}_\sigma)
\end{align}
where $\underline{s}_\sigma$ and $\underline{\nu}_\sigma$ are positive scalar hyper-parameters of the scale and shape respectively.
\begin{itemize}
\item Derive the full-conditional posterior distribution of the parameter $\underline{\sigma}^2_\alpha$ given parameter $\sigma^2$ and hyper-parameters $\underline{s}$, $\underline{\nu}$, $\underline{s}_\sigma$, and $\underline{\nu}_\sigma$ denoted by $p\left(\underline{\sigma}^2_\alpha\mid \sigma^2, \underline{s}, \underline{\nu},\underline{s}_\sigma,\underline{\nu}_\sigma\right)$. Show your workings.
\end{itemize}

\newpage
\bigskip\item[Exercise 7.] Consider the following linear regression model for a scalar random variable $y_t$, $K$ explanatory variables denoted by $x_{k.t}$ and with the corresponding regression parameters $\beta_k$ for $k=1,\dots, K$, and the error term variance $\sigma^2$:
\begin{align}
y_t &= \beta_1 x_{1.t} + \dots + \beta_K x_{K.t} + \epsilon_t\\
\epsilon_t \mid x_{1.t}, \dots, x_{K.t} &\sim iid\mathcal{N}\left(0, \sigma^2\right)
\end{align}
\begin{itemize}
\item Write out the model in a matrix notation.
\item State the distribution of the error term vector explicitly.
\item State the predictive density implied by the model for the dependent variable vector given the explanatory variables.
\item Write out the likelihood function for the model.
\end{itemize}


\bigskip\item[Exercise 8.] Consider the linear Gaussian model from \textbf{Exercise 7} presented in the matrix notation. Assume a joint zero-mean normal inverted gamma 2 prior distribution for the parameters of the model, a $K$-vector vector $\boldsymbol\beta = (\beta_1,\dots, \beta_K)'$ and a scalar $\sigma^2$ stated as:
\begin{align}
\boldsymbol\beta, \sigma^2 \mid \underline{\sigma}_\beta^2, \underline{s}, \underline{\nu} \sim\mathcal{NIG}2\left(\mathbf{0}_K, \underline{\sigma}_\beta^2 \mathbf{I}_K, \underline{s}, \underline{\nu} \right)
\end{align}
where $\mathbf{I}_K$ in the identity matrix of order $K$, and  $\underline{\sigma}_\beta^2$, $\underline{s}$, and $\underline{\nu}$ denote positive scalar hyper-parameters.
\begin{itemize}
\item Derive the full-conditional posterior distribution of the parameter vector $\boldsymbol\beta$ given data, as well as parameter $\sigma^2$ and hyper-parameters $\underline{\sigma}^2_\beta$, $\underline{s}$, and $\underline{\nu}$, denoted by $p\left(\boldsymbol\beta\mid data, \sigma^2, \underline{\sigma}^2_\beta, \underline{s}, \underline{\nu}\right)$. Show your workings.
\end{itemize}
\item[a pdf of the normal inverted gamma 2 distribution] for an $N$-vector $\mathbf{X}$ and positive real-values scalar parameter $\sigma^2$ with mean parameter $\bar{\boldsymbol\mu}$, variance parameter $\mathbf\Sigma$ scale $s$ and shape $\nu$
\begin{align}
\Gamma\left(\frac{\nu}{2}\right)^{-1}\left(\frac{s}{2}\right)^{\frac{\nu}{2}}(2\pi)^{-\frac{N}{2}}\det(\mathbf\Sigma)^{-\frac{1}{2}}\left(\sigma^2\right)^{-\frac{\nu+N+2}{2}}\exp\left\{ -\frac{1}{2}\frac{1}{\sigma^2}\left[ s+(\mathbf{X}-\boldsymbol\mu)'\mathbf\Sigma^{-1}(\mathbf{X}-\boldsymbol\mu) \right]\right\}
\end{align}


\bigskip\item[Exercise 9.] Consider a linear regression model from \textbf{Exercise 7} with the normal inverted gamma~2 joint prior distribution for parameters $\boldsymbol\beta$ and $\sigma^2$ as in \textbf{Exercise 8}. Suppose that you are interested in estimating the prior shrinkage hyper-parameter $\underline{\sigma}_\beta^2$. For that purpose, assume the following inverted gamma 2 prior distribution for this hyper-parameter:
\begin{align}
\underline{\sigma}^2_\beta \mid \underline{s}_\sigma,\underline{\nu}_\sigma &\sim\mathcal{IG}2(\underline{s}_\sigma,\underline{\nu}_\sigma)
\end{align}
where $\underline{s}_\sigma$ and $\underline{\nu}_\sigma$ are positive scalar hyper-parameters of the scale and shape 
\begin{itemize}
\item Given this setup scrutinise the Gibbs sampler for the parameters $\boldsymbol\beta$, $\sigma^2$, and $\underline{\sigma}^2_\beta$. Describe all of the steps of the sampler and make certain that you describe each of its iterations sufficiently to facilitate writing a computer algorithm using your description. Do not derive the full-conditional posterior distributions, but clearly state them making certain that the notation is clear for which parameter a particular distribution is defined and that all of the objects on which you condition the distributions are listed.
\end{itemize}


\end{description} 



\end{adjustwidth}

\end{document}


