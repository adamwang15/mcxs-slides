	\documentclass[notes,blackandwhite,mathsans]{beamer}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{multirow,pxfonts}
\usepackage{cmbright}
\usepackage{color}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{changepage}

\usepackage[T1]{fontenc}
\fontencoding{T1}  
\usepackage[utf8]{inputenc}


\usefonttheme{default}
\setbeamercovered{invisible}
\beamertemplatenavigationsymbolsempty

\makeatletter
\setbeamertemplate{footline}
{
  \leavevmode
  \hbox{
  \begin{beamercolorbox}[wd=0.97\paperwidth,ht=2.25ex,dp=2ex,right]{}
{\color{mcxs3} \insertframenumber{} / \inserttotalframenumber}
  \end{beamercolorbox}}%
}



\definecolor{mcxs1}{HTML}{05386B}
\definecolor{mcxs2}{HTML}{379683}
\definecolor{mcxs3}{HTML}{5CDB95}
\definecolor{mcxs4}{HTML}{8EE4AF}
\definecolor{mcxs5}{HTML}{EDF5E1}
\setbeamercolor{frametitle}{fg=mcxs2}
\AtBeginDocument{\color{mcxs1}}

%\setbeamercolor{itemize text}{fg=mcxs5}
\setbeamercolor{itemize item}{fg=mcxs1}
\setbeamercolor{itemize subitem}{fg=mcxs2}
\setbeamercolor{enumerate item}{fg=mcxs1}
\setbeamercolor{description item}{fg=mcxs1}

\setbeamertemplate{itemize item}[triangle]
\setbeamertemplate{itemize subitem}[circle]



\begin{document}
%\fontfamily{pag}\selectfont
%\setbeamerfont{title}{family=\fontfamily{pag}\selectfont}
%\setbeamerfont{frametitle}{family=\fontfamily{pag}\selectfont}
%\setbeamerfont{framesubtitle}{family=\fontfamily{pag}\selectfont}



\begin{frame}
\centering\includegraphics[scale=1.87]{mcxs.png}
\end{frame}





{\setbeamercolor{background canvas}{bg=mcxs3}
\begin{frame}

\vspace{1cm}
\begin{tabular}{rl}
&\textbf{\LARGE\color{mcxs1} Macroeconometrics}\\[8ex]
\textbf{\Large Lecture 2}&\textbf{\Large\color{mcxs5}Maximum Likelihood Estimation}\\[19ex]
&\textbf{Tomasz Wo\'zniak}\\[1ex]
&{\small\color{mcxs5} Department of Economics}\\
&{\small\color{mcxs5}University of Melbourne}
\end{tabular}

\end{frame}
}



{\setbeamercolor{background canvas}{bg=mcxs2}
\begin{frame}

\vspace{1.5cm} \textbf{\color{mcxs3}Likelihood function}

\bigskip\textbf{\color{mcxs1}Estimation: analytical solution}

\bigskip\textbf{\color{mcxs3}Properties of the maximum likelihood estimator}

%\bigskip\textbf{\color{mcxs3}Maximum likelihood inference}


%\vspace{1cm} Useful readings: \\ \footnotesize
%\smallskip{\color{mcxs3}Harris, Hurn, \& Martin (2012) Chapter 1: The Maximum Likelihood Principle, Econometric Modelling with Time Series\\[1ex]
%Harris, Hurn, \& Martin (2012) Chapter 2: Properties of Maximum Likelihood Estimators, Econometric Modelling with Time Series}

\end{frame}
}



%{\setbeamercolor{background canvas}{bg=mcxs2}
%\begin{frame}
%
%\bigskip\textbf{\color{mcxs1}Objectives.}
%\begin{itemize}[label=$\blacktriangleright$]
%\item {\color{mcxs1}To learn the basics of the maximum likelihood method}
%\item {\color{mcxs1}To derive analytical solutions for a simple model}
%\item {\color{mcxs1}To look at theoretical results that enable hypothesis testing}
%\end{itemize}
%
%\bigskip\textbf{\color{mcxs3}Learning outcomes.}
%\begin{itemize}[label=$\blacktriangleright$]
%\item {\color{mcxs3}Setting up an optimisation problem}
%\item {\color{mcxs3}Using calculus to provide closed-form solutions}
%\item {\color{mcxs3}Constructing a statistical test of a hypothesis}
%\end{itemize}
%
%\end{frame}
%}






{\setbeamercolor{background canvas}{bg=mcxs3}
\begin{frame}

\begin{adjustwidth}{-0.5cm}{0cm}
%\FlushLeft
\vspace{8.3cm}\Large
\textbf{{\color{mcxs1}Likelihood} {\color{mcxs2}function}}
\end{adjustwidth}

\end{frame}
}




\begin{frame}
\frametitle{A simple model}

\textbf{Univariate linear regression model.}\\

\begin{align*}
y_t &= \beta x_t + \epsilon_t\\
\epsilon_t|x_t &\sim iid\mathcal{N}\left(0, \sigma^2\right)
\end{align*}

\bigskip\begin{description}\small
\item[$y_t$] {\color{mcxs3}-- dependent variable}
\item[$\theta=\left(\beta, \sigma^2\right)'$] {\color{mcxs3}-- a $2\times1$ vector of unknown parameters }
\item[$x_t$] {\color{mcxs3}-- explanatory variable}
\item[$\epsilon_t$] {\color{mcxs3}-- error term}
\item[$T$] {\color{mcxs3}-- sample size and} $t\in\{1,\dots,T\}$
\end{description}

\end{frame}



\begin{frame}
\frametitle{A simple model}

\bigskip\textbf{The model in matrix notation.}
\begin{align*}
Y &= \beta X + E\\
E|X &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2I_T\right)
\end{align*}



\textbf{Data matrices.}

$$ \underset{\color{mcxs3}(T\times1)}{Y} = \begin{bmatrix} y_1\\ \vdots \\ y_T \end{bmatrix} \qquad \underset{\color{mcxs3}(T\times1)}{X} = \begin{bmatrix} x_1\\ \vdots \\ x_T \end{bmatrix} \qquad \underset{\color{mcxs3}(T\times1)}{E} = \begin{bmatrix} \epsilon_1\\ \vdots \\ \epsilon_T \end{bmatrix} $$

\end{frame}





\begin{frame}
\frametitle{Predictive density}

{\color{mcxs3}\small Assumptions about the model and the conditional distribution of the error term determine the {\color{mcxs2}predictive distribution of data} given the parameters and explanatory variables:}

\begin{center}
\begin{tabular}{rlcrl}
$Y$ &$= \beta X + E$ &\multirow{2}{*}{$\Rightarrow$}& $Y$ &$= \beta X + E$\\
$E|X$ &$\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2I_T\right)$ && $Y|X$ &$\sim\mathcal{N}\left(\beta X, \sigma^2I_T\right)$
\end{tabular}
\end{center}

\end{frame}





\begin{frame}
\frametitle{Predictive density}

\textbf{Linear transformation of a normal vector.}

\small
{\color{mcxs3} Let a random vector} $Y$ {\color{mcxs3}follow an $N$-variate normal distribution with the mean vector} $\mu$ {\color{mcxs3}and the covariance matrix} $\Sigma$:
$$ Y\sim\mathcal{N}_N\left( \mu, \Sigma \right) $$
{\color{mcxs3}Let} $Z = AY + b$. {\color{mcxs3}Then:}
$$ Z\sim\mathcal{N}_N\left( A\mu + b, A\Sigma A' \right) $$
\end{frame}





\begin{frame}
\frametitle{Likelihood function}

{\color{mcxs3}A {\color{mcxs2}likelihood function} is equivalent to the conditional distribution of the data, given the parameters of the model.\\ \smallskip
However, for the purpose of the estimation and after plugging in data} $Y$ {\color{mcxs3}and} $X$ {\color{mcxs3}we treat it as a function of unknown parameters} $\theta$.

\small
\begin{align*}
L\left(\theta|Y,X\right) & = L\left(\beta, \sigma^2|Y,X\right)= p\left(Y|X, \beta, \sigma^2 \right) = \mathcal{N}_T\left(\beta X, \sigma^2I_T\right)\\
& = (2\pi)^{-\frac{T}{2}} \det\left( \sigma^2 I_T \right)^{-\frac{1}{2}}\exp\left\{ -\frac{1}{2}(Y-\beta X)'\left( \sigma^2 I_T \right)^{-1} (Y-\beta X) \right\}\\
&= (2\pi)^{-\frac{T}{2}} \left( \sigma^2 \right)^{-\frac{T}{2}}\exp\left\{ -\frac{1}{2}\frac{1}{\sigma^2}(Y-\beta X)'(Y-\beta X) \right\}
\end{align*}

\smallskip\textbf{Useful operations.}\\
{\color{mcxs3}Let} $c$ {\color{mcxs3}be a scalar and} $X$ {\color{mcxs3}an $N\times N$ matrix. Then} $\det(cX)=c^N\det(X)$.

\end{frame}





\begin{frame}
\frametitle{The likelihood principle}

{\color{mcxs2}All of the information about the parameters of the model} $\theta$ {\color{mcxs2}that is embedded in the dataset} $Y$ {\color{mcxs2}is captured by the likelihood function.}

\end{frame}





\begin{frame}
\frametitle{log-likelihood function}

{\color{mcxs3}To derive the analytical solution and to be able to evaluate the likelihood function for any values  of } $\theta\in\Theta${\color{mcxs3}, the logarithmic transformation is applied through which the log-likelihood function is obtained.} $\Theta$ {\color{mcxs3}denotes the parameter space, that is, a~set of all admissible values of the parameters.}

\small
\begin{align*}
l\left(\theta|Y,X\right) & = \ln L\left(\theta|Y,X\right)\\
&= -\frac{T}{2}\ln(2\pi) -\frac{T}{2}\ln\sigma^2 -\frac{1}{2}\frac{1}{\sigma^2}(Y-\beta X)'(Y-\beta X) \\
&= {\color{mcxs3}-\frac{T}{2}\ln(2\pi) -\frac{T}{2}\ln\sigma^2 -\frac{1}{2}\frac{1}{\sigma^2}(Y'Y -\beta 2X'Y + \beta^2 X'X) }
\end{align*}

\end{frame}



\begin{frame}
\frametitle{The maximum likelihood estimator}

{\color{mcxs3}The maximum likelihood estimator ({\color{mcxs2}MLE}) of } $\theta${\color{mcxs3}, denoted by} $\hat\theta${\color{mcxs3},\\ is found where the log-likelihood function is at its maximum:}

\small
\begin{equation*}
\hat\theta = \underset{\theta\in\Theta}{\text{\color{mcxs3} argmax}}\hspace{0.1cm} l\left(\theta|Y,X\right) 
\end{equation*}

\vspace{1cm}{\color{mcxs3}Finding the maximum of the log-likelihood function is equivalent to finding the maximum of the likelihood function as the logarithm is a monotonic transformation that preserves local optima.}

\bigskip{\color{mcxs3}The derivations and properties are feasible under} {\color{mcxs2}regularity conditions}.
\end{frame}




\begin{frame}
\frametitle{Regularity conditions}

{\color{mcxs3}Let} $\theta_0$ {\color{mcxs3}denote the} {\color{mcxs2}true values} {\color{mcxs3}of the parameters} $\theta$.

\begin{description}[leftmargin=0.65cm] \small
\item[A1] Existence\\ \footnotesize
{\color{mcxs3}The following expectation exists:}
$$ \mathbb{E}[l(\theta|Y,X)] = \int_{-\infty}^{\infty} l(\theta|Y,X) L(\theta_0|Y,X)dY\footnote{\tiny \color{mcxs3}Here, $L(\theta_0|Y,X)$ is the trye probability density funcion of the data evaluated at the true parameter value $\theta_0$, and the integral is taken over the entire range of possible values of the data.} $$ \small 

\item[A2] Convergence\\ \footnotesize
$l(\theta|Y,X)$ {\color{mcxs3}converges in probability to its expectation uniformly in} $\theta$.
$$\text{plim } l(\theta|Y,X)=\mathbb{E}[l(\theta|Y,X)]$$ \small
\item[A3] Continuity \\ \footnotesize
$l(\theta|Y,X)$ {\color{mcxs3}is continuous in} $\theta$. \small
\item[A4] Differentiability\\ \footnotesize
$l(\theta|Y,X)$ {\color{mcxs3}is at least twice differentiable in an open interval around} $\theta_0$.\small
\item[A5]Interchangeability\\ \footnotesize
{\color{mcxs3}The differentiation and integration order of} $l(\theta|Y,X)$ {\color{mcxs3}is interchangeable.}
\end{description}

\end{frame}




{\setbeamercolor{background canvas}{bg=mcxs2}
\begin{frame}

\begin{adjustwidth}{-0.5cm}{0cm}
%\FlushLeft
\vspace{8.3cm}\Large
\textbf{{\color{mcxs1}Estimation:} {\color{mcxs4}analytical solution}}
\end{adjustwidth}

\end{frame}
}



\begin{frame}
\frametitle{Estimation: analytical solution}

{\color{mcxs3}To derive the analytical solution of {\color{mcxs2}MLE} use calculus.}

\bigskip\textbf{Gradient vector.}
\begin{equation*}
\underset{\color{mcxs3}(2\times1)}{G(\theta)} = \frac{\partial l\left(\theta|Y,X\right) }{\partial\theta} 
= \begin{bmatrix} \frac{\partial l\left(\theta|Y,X\right) }{\partial\beta} \\ \frac{\partial l\left(\theta|Y,X\right) }{\partial\sigma^2} \end{bmatrix}
\end{equation*}

\bigskip{\color{mcxs3}The MLE occurs where all of the gradients are equal to zero:}
\begin{equation*}
G(\hat\theta) = \frac{\partial l\left(\theta|Y,X\right) }{\partial\theta} \bigg|_{\theta=\hat\theta}=\mathbf{0}_2
\end{equation*}

\end{frame}




\begin{frame}
\frametitle{Estimation: analytical solution}

\bigskip\normalsize\textbf{Hessian matrix.}
\begin{equation*}
\underset{\color{mcxs3}(2\times2)}{H(\theta)} = \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial\theta\partial\theta'} 
= \begin{bmatrix} \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial^2\beta} & \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial\beta\partial\sigma^2} \\ \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial\sigma^2\partial\beta} & \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial^2\sigma^2} \end{bmatrix}
\end{equation*}

\bigskip{\color{mcxs3}The MLE maximizes the log-likelihood function when the Hessian matrix ordinate at the MLE:}
\begin{equation*}
H(\hat\theta) = \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial\theta\partial\theta'} \bigg|_{\theta=\hat\theta}
\end{equation*}
{\color{mcxs2}is negative definite}.

\end{frame}






\begin{frame}
\frametitle{Estimation: analytical solution}

\textbf{The gradient.}
\begin{equation*}
G(\theta) = \begin{bmatrix} \frac{\partial l\left(\theta|Y,X\right) }{\partial\beta} \\ \frac{\partial l\left(\theta|Y,X\right) }{\partial\sigma^2} \end{bmatrix}
= \begin{bmatrix} 
-\frac{1}{2}\frac{1}{\sigma^2}\left( -2X'Y + \beta 2X'X \right) \\
-\frac{T}{2}\frac{1}{\sigma^2} + \frac{1}{2}\frac{1}{\left(\sigma^2\right)^2}(Y-\beta X)'(Y-\beta X)
 \end{bmatrix}
\end{equation*}

\bigskip\textbf{Necessary condition.}
\begin{equation*}
G(\hat\theta) = \begin{bmatrix} \frac{\partial l\left(\theta|Y,X\right) }{\partial\beta}\big|_{\theta=\hat\theta} \\ \frac{\partial l\left(\theta|Y,X\right) }{\partial\sigma^2}\big|_{\theta=\hat\theta} \end{bmatrix} = \begin{bmatrix} 0\\0\end{bmatrix}
\end{equation*}
\begin{equation*}
\begin{bmatrix} 
-\frac{1}{2}\frac{1}{\hat\sigma^2}\left( -2X'Y + \hat\beta 2X'X \right) \\
-\frac{T}{2}\frac{1}{\hat\sigma^2} + \frac{1}{2}\frac{1}{\left(\hat\sigma^2\right)^2}(Y-\hat\beta X)'(Y-\hat\beta X)
 \end{bmatrix} = \begin{bmatrix} 0\\0\end{bmatrix}
\end{equation*}

\end{frame}


\begin{frame}
\frametitle{Estimation: analytical solution}

\textbf{The first equation.}
\begin{align*}
0 &= -\frac{1}{2}\frac{1}{\hat\sigma^2}\left( -2X'Y + \hat\beta 2X'X \right)  \\
\hat\beta X'X  &= X'Y\\
\hat\beta &= (X'X)^{-1}X'Y
\end{align*}


\bigskip\textbf{The second equation.}
\begin{align*}
0 &= -\frac{T}{2}\frac{1}{\hat\sigma^2} + \frac{1}{2}\frac{1}{\left(\hat\sigma^2\right)^2}(Y-\hat\beta X)'(Y-\hat\beta X) \Bigg\slash \cdot\frac{2\left( \hat\sigma^2\right)^2}{T} \\
\hat\sigma^2 &= \frac{1}{T}(Y-\hat\beta X)'(Y-\hat\beta X)
\end{align*}

\end{frame}





\begin{frame}
\frametitle{Estimation: analytical solution}

\normalsize\textbf{The Hessian matrix.}\scriptsize
\begin{equation*}
H(\theta) = \begin{bmatrix} \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial^2\beta} & \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial\beta\partial\sigma^2} \\ \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial\sigma^2\partial\beta} & \frac{\partial^2 l\left(\theta|Y,X\right) }{\partial^2\sigma^2} \end{bmatrix}
=
\begin{bmatrix} 
-\frac{1}{\sigma^2}X'X & -\frac{1}{\left(\sigma^2\right)^2}X'(Y-\beta X)\\
& \frac{T}{2}\frac{1}{\left(\sigma^2\right)^2} - \frac{1}{\left(\sigma^2\right)^3}(Y-\beta X)'(Y-\beta X)
 \end{bmatrix}
\end{equation*}



\bigskip\normalsize\textbf{Sufficient condition.}\\
$H(\hat\theta)$ {\color{mcxs2}must be negative definite}.

\footnotesize\begin{equation*}
H(\hat\theta) = \begin{bmatrix} 
-\frac{1}{\hat\sigma^2}X'X & -\frac{1}{\left(\hat\sigma^2\right)^2}X'\hat{E}\\
& \frac{1}{2}\frac{T}{\left(\hat\sigma^2\right)^2} - \frac{1}{\left(\hat\sigma^2\right)^3}\hat{E}'\hat{E}
 \end{bmatrix}
 = \begin{bmatrix} 
-\frac{1}{\hat\sigma^2}X'X & 0 \\
0 & -\frac{1}{2}\frac{T}{\left(\hat\sigma^2\right)^2} 
 \end{bmatrix}
\end{equation*}

\normalsize\bigskip{\color{mcxs3}where} $\frac{1}{T}X'\hat{E}=0$ {\color{mcxs3}(exogeneity condition), and} $\hat\sigma^2=\frac{1}{T}\hat{E}'\hat{E}$.

\end{frame}



\begin{frame}
\frametitle{Estimation: analytical solution}

\textbf{Negative definite matrix.}\small

{\color{mcxs3}A symmetric} $N\times N$ {\color{mcxs3}matrix} $Z$ {\color{mcxs3}is {\color{mcxs2}negative definite} if} $z'Zz<0$ {\color{mcxs3}for all} $N\times1$ {\color{mcxs3}vectors} $z\neq\mathbf{0}_N$.

\bigskip{\color{mcxs3}A symmetric} $2\times 2$ {\color{mcxs3}matrix} $Z$ {\color{mcxs3}is negative definite if} $Z_{11}<0$ {\color{mcxs3}and} $\det(Z)>0$.

\bigskip{\color{mcxs3}Since} $-\frac{1}{\hat\sigma^2}X'X<0$ {\color{mcxs3}and} $\det(H(\hat\theta))= \frac{1}{2}\frac{1}{\left(\hat\sigma^2\right)^3}X'X >0 $ {\color{mcxs2}the Hessian matrix is negative definite}.


\bigskip{\color{mcxs3}The MLE:}
$$
\hat\theta= \begin{bmatrix} \hat\beta\\ \hat\sigma^2 \end{bmatrix} = \begin{bmatrix} (X'X)^{-1}X'Y\\ \frac{1}{T}(Y-\hat\beta X)'(Y-\hat\beta X) \end{bmatrix}
$$
{\color{mcxs3}is a point at which the log-likelihood achieves the global maximum.}
\end{frame}








{\setbeamercolor{background canvas}{bg=mcxs1}
\begin{frame}

\begin{adjustwidth}{-0.5cm}{0cm}
%\FlushLeft
\vspace{8.3cm}\Large
\textbf{{\color{mcxs2}Properties of the} {\color{mcxs4}MLE}}
\end{adjustwidth}

\end{frame}
}





\begin{frame}
\frametitle{MLE properties: consistency}

\textbf{Consistency.}\\
{\color{mcxs3}The probability limit of the MLE when the sample size increases is the vector of the true parameter values.}
$$ \text{plim }\hat\theta = \theta_0 $$

\bigskip\textbf{Definition of plim.}

$$ \lim_{T\rightarrow\infty} \Pr\left[ |\hat\theta - \theta_0|<c\right] = 1,\text{\color{mcxs3} for any }c>0 $$


\end{frame}




\begin{frame}
\frametitle{MLE properties: asymptotic normality}

\textbf{Normality.}\\
{\color{mcxs3}The MLE converges in distribution to the following normal distribution when the sample size goes to infinity.}
$$ \sqrt{T}\left(\hat\theta - \theta_0\right) \overset{d}{\rightarrow} \mathcal{N}\left( \mathbf{0}, \Omega(\theta_0) \right) $$
{\color{mcxs3}where} $\Omega(\theta_0)$ {\color{mcxs3}is the inverse of the Fisher information matrix:}
$$\Omega(\theta_0) = TI^{-1}(\theta_0) = T\left[ -\mathbb{E}\left[H(\theta_0)\right] \right]^{-1}$$

\bigskip\textbf{Asymptotic distribution.}

$$ \hat\theta\overset{a}{\sim}\mathcal{N}\left(\theta_0,\frac{1}{T}\Omega(\theta_0)\right) $$


\end{frame}





\begin{frame}
\frametitle{MLE properties: asymptotic normality}

\textbf{Estimator of covariance of $\hat\theta$.}
\begin{align*} 
\widehat{Var}(\hat\theta) &= \frac{1}{T}\Omega(\theta)|_{\theta=\hat\theta} =\left[ -H(\theta) \right]^{-1}\big|_{\theta=\hat\theta}\\
&= \begin{bmatrix} 
\hat\sigma^2(X'X)^{-1} & 0 \\
0 & \frac{2\left(\hat\sigma^2\right)^2}{T}
 \end{bmatrix}
\end{align*} 

\bigskip\textbf{Estimation standard erros.}
\begin{align*} 
\hat{se}\left(\hat\beta\right) &= \hat\sigma(X'X)^{-\frac{1}{2}}\\
\hat{se}\left(\hat\sigma^2\right) &=\sqrt{\frac{2}{T}}\hat\sigma^2
\end{align*} 

\end{frame}





\begin{frame}
\frametitle{MLE properties: efficiency and invariance}

\textbf{Efficiency.}\\
{\color{mcxs3}The covariance of the MLE hits the Rao-Cramer lower bound:}
$$\frac{1}{T}\Omega(\theta_0) =  I^{-1}(\theta_0)$$
{\color{mcxs3}No other estimator has lower standard errors than the MLE.}

\bigskip\textbf{Invariance.}\\
{\color{mcxs3}The MLE of a continuous and differentiable function of parameters} $g(\theta)$ {\color{mcxs3}is given by:}
$$ \widehat{g(\theta)} = g(\theta)|_{\theta=\hat\theta}=g(\hat\theta) $$
{\color{mcxs3}Example:} $\hat\sigma = \sqrt{\hat\sigma^2}$

\end{frame}





%{\setbeamercolor{background canvas}{bg=mcxs4}
%\begin{frame}
%
%\begin{adjustwidth}{-0.5cm}{0cm}
%\FlushLeft
%\vspace{8.3cm}\Large
%\textbf{{\color{mcxs5}Maximum likelihood} {\color{mcxs2}inference}}
%\end{adjustwidth}
%
%\end{frame}
%}
%
%
%
%\begin{frame}
%\frametitle{Wald test: notation}
%\small
%
%{\color{mcxs3}Let} $\mathbf{R}(\theta): \mathbb{R}_k\rightarrow\mathbb{R}_l$ {\color{mcxs3}be a} $l${\color{mcxs3}-variate function of} $k\times 1$ {\color{mcxs3}vector of parameters, such that:}
%$$ \mathbf{R}(\theta) = \begin{bmatrix} \mathbf{R}_1(\theta) \\ \vdots \\ \mathbf{R}_l(\theta) \end{bmatrix} $$
%
%{\color{mcxs3}Let} $\mathbf{J}(\theta)$ {\color{mcxs3}be a} $l\times k$ {\color{mcxs2}Jacobian matrix} {\color{mcxs3}such that:}
%$$ \mathbf{J}(\theta) = \begin{bmatrix} \frac{\partial\mathbf{R}_1(\theta)}{\partial\theta_1} & \cdots & \frac{\partial\mathbf{R}_1(\theta)}{\partial\theta_k} \\ \vdots & & \vdots \\ \frac{\partial\mathbf{R}_l(\theta)}{\partial\theta_1} & \cdots & \frac{\partial\mathbf{R}_l(\theta)}{\partial\theta_k} \end{bmatrix} $$
%\end{frame}
%
%
%\begin{frame}
%\frametitle{delta method}
%\small
%
%{\color{mcxs3}Let} $\hat\theta$ {\color{mcxs3}be a vector of {\color{mcxs2}normally distributed} parameters with an asymptotic covariance matrix} $Var[\hat\theta]$.
%
%\vspace{1cm}\textbf{Problem:} {\color{mcxs3}What is an {\color{mcxs2}asymptotic} covariance matrix of an estimator of the  function of parameters} $\mathbf{R}(\hat\theta)$?
%
%\vspace{1cm}\textbf{Soluton:} {\color{mcxs3}Use} delta method {\color{mcxs3}to obtain:}
%$$ Var\left[ \mathbf{R}(\hat\theta) \right] = \mathbf{J}(\hat\theta)Var[\hat\theta]\mathbf{J}(\hat\theta)^{'} $$
%
%\end{frame}
%
%
%\begin{frame}
%\frametitle{Wald test}
%\small
%{\color{mcxs3}Estimation of the unrestricted model is required. The estimator of parameters for this model is denoted by} $\hat\theta$.
%
%\vspace{0.3cm}\textbf{Hypotheses:}
%\begin{align*}
%\mathcal{H}_0: & \mathbf{R}(\theta) = \mathbf{r}\\
%\mathcal{H}_1: & \mathbf{R}(\theta) \neq \mathbf{r}
%\end{align*}
%
%\textbf{Test statistic:}
%$$ \mathbb{W} = \left( \mathbf{R}(\hat\theta) - \mathbf{r} \right)^{'}\left(\mathbf{J}(\hat\theta)Var[\hat\theta]\mathbf{J}(\hat\theta)^{'}\right)^{-1}\left( \mathbf{R}(\hat\theta) - \mathbf{r} \right) $$
%
%\vspace{0.3cm}\textbf{Asymptotic distribution:} $\mathbb{W}\sim\chi^2(l)$
%
%\vspace{0.3cm}{\color{mcxs2}Reject} {\color{mcxs3}the null hypothesis if} $\mathbb{W}>\chi^2_{\alpha}(l)$, {\color{mcxs3}where} $\chi^2_{\alpha}(l)$ {\color{mcxs3}is a} $100(1-\alpha)$th {\color{mcxs3}percentile of the} $\chi^2$ {\color{mcxs3}distribution with} $l$ {\color{mcxs3}degrees of freedom.}
%
%\end{frame}
%
%
%
%
%
%\begin{frame}
%\frametitle{Likelihood ratio test}
%\small
%{\color{mcxs3}Estimation of the restricted and unrestricted model is required.}
%
%\vspace{0.3cm}\textbf{Hypotheses:}
%\begin{align*}
%\mathcal{H}_0: & \mathbf{R}(\theta) = \mathbf{r}\\
%\mathcal{H}_1: & \mathbf{R}(\theta) \neq \mathbf{r}
%\end{align*}
%
%\textbf{Test statistic:}
%$$ \mathbb{LR} = 2\left( l(\hat{\theta}|Y,X) - l_R(\hat\theta_R|Y,X)\right) $$
%{\color{mcxs3}where} $l(\hat{\theta}|Y,X)$ {\color{mcxs3}- is the value of the log-likelihood function for the} {\color{mcxs2}unrestricted model}, {\color{mcxs3}and} $l_R(\hat\theta_R|Y,X)$ {\color{mcxs3}- is the value of the log-likelihood function for the {\color{mcxs2}restricted model}}
%
%\vspace{0.3cm}\textbf{Asymptotic distribution:} $\mathbb{LR}\sim\chi^2(l)$
%
%\vspace{0.3cm}{\color{mcxs3}Reject the null hypothesis if} $\mathbb{LR}>\chi^2_{\alpha}(l)${\color{mcxs3}, where} $\chi^2_{\alpha}(l)$ {\color{mcxs3}is a} $100(1-\alpha)$th {\color{mcxs3}percentile of the} $\chi^2$ {\color{mcxs3}distribution with $l$ degrees of freedom.}
%
%\end{frame}
%
%
%
%
%
%
%\begin{frame}
%\frametitle{Lagrange multiplier test}
%\small
%{\color{mcxs3}Estimation of the restricted model is required. The estimated parameter vector for this model is denoted by} $\hat\theta_R$. {\color{mcxs3}Note that it contains the values of restricted parameters.}
%
%\vspace{0.3cm}\textbf{Hypotheses:}
%\begin{align*}
%\mathcal{H}_0: & \mathbf{R}(\theta) = \mathbf{r}\\
%\mathcal{H}_1: & \mathbf{R}(\theta) \neq \mathbf{r}
%\end{align*}
%
%\textbf{Test statistic:}
%$$ \mathbb{LM} = TG(\hat\theta_R)'\Omega(\hat\theta_R)G(\hat\theta_R) $$
%{\color{mcxs3}where} $G(\theta)$ {\color{mcxs3}and} $\Omega(\theta)$ {\color{mcxs3}are the gradient vector and the information matrix derived for the log-likelihood function of the} {\color{mcxs2}unrestricted model.}
%
%\vspace{0.3cm}\textbf{Asymptotic distribution:} $\mathbb{LM}\sim\chi^2(l)$
%
%\vspace{0.3cm}{\color{mcxs3}Reject the null hypothesis if} $\mathbb{LM}>\chi^2_{\alpha}(l)${\color{mcxs3}, where} $\chi^2_{\alpha}(l)$ {\color{mcxs3}is a} $100(1-\alpha)$th {\color{mcxs3}percentile of the} $\chi^2$ {\color{mcxs3}distribution with $l$ degrees of freedom.}
%
%\end{frame}



{\setbeamercolor{background canvas}{bg=mcxs3}
\begin{frame}{Maximum Likelihood Estimation}

\textbf{Maximum likelihood estimation and inference} is a powerful tool for data analysis. 

\bigskip It is still one of the most frequently used methods in macroeconometrics as long as its application is \textbf{numerically feasible}.

\bigskip Some specialised techniques, such as appropriately set \textbf{numerical optimization} and \textbf{concentration} of the likelihood function that are presented later during this subject, make its application simpler for some models.

\end{frame}
}






\end{document} 